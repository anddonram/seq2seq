#Seq2seq
Some models (Attention one) require recurrentshop. See for more instructions:

https://github.com/farizrahman4u/seq2seq



#References


https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py#L153

https://www.tensorflow.org/tutorials/seq2seq

https://keras.io/getting-started/functional-api-guide/#multi-input-and-multi-output-models

https://machinelearnings.co/deep-spelling-9ffef96a24f6

https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence-prediction-keras/

https://github.com/google/seq2seq

https://github.com/farizrahman4u/seq2seq

And more...
